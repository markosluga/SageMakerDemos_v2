{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training with SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo will walk you through how you can utilize Amazon SageMaker distributed training libraries within SageMaker Studio.\n",
    "\n",
    "Before proceeding with the notebook, copy the `DefaultBucket` output value from the lab instructions window and paste it between the quotes in the following cell. Executing this cell ensures that SageMaker will use the lab created bucket throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = '**REPLACE ME**'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment setup stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, session\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(f\"Designated SageMaker role: {role}\")\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "sagemaker_session._default_bucket = bucket_name\n",
    "print(f\"SageMaker default bucket:\\n{sagemaker_session._default_bucket}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the PyTorch estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"pytorch-smdataparallel-mnist\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train_pytorch_smdataparallel_mnist.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.8.1\",\n",
    "    py_version=\"py36\",\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    # may need to use g4dn.8xlarge / g4dn.16xlarge / g4dn.12xlarge\t\n",
    "    instance_type=\"ml.p3dn.24xlarge\",\n",
    "    # instance_type=\"g4dn.8xlarge\",\n",
    "    # cannot use g4dn with smdataparallel\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(\"Storing {} as model_data\".format(model_data))\n",
    "# TODO refactor\n",
    "%store model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a saved model from a previous notebook run's stored variable\n",
    "# TODO refactor\n",
    "%store -r model_data\n",
    "\n",
    "# If no model was found, set it manually here.\n",
    "# model_data = 's3://sagemaker-us-west-2-XXX/pytorch-smdataparallel-mnist-2020-10-16-17-15-16-419/output/model.tar.gz'\n",
    "\n",
    "print(\"Using this model: {}\".format(model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the test set\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from packaging.version import Version\n",
    "\n",
    "# Set the source to download MNIST data from\n",
    "TORCHVISION_VERSION = \"0.9.1\"\n",
    "if Version(torchvision.__version__) < Version(TORCHVISION_VERSION):\n",
    "    # Set path to data source and include checksum key to make sure data isn't corrupted\n",
    "    datasets.MNIST.resources = [\n",
    "        (\n",
    "            \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\",\n",
    "            \"f68b3c2dcbeaaa9fbdd348bbdeb94873\",\n",
    "        ),\n",
    "        (\n",
    "            \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\",\n",
    "            \"d53e105ee54ea40749a09fcbcd1e9432\",\n",
    "        ),\n",
    "        (\n",
    "            \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\",\n",
    "            \"9fb629c4189551a2d022fa330f9573f3\",\n",
    "        ),\n",
    "        (\n",
    "            \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\",\n",
    "            \"ec29112dd5afa0611ce80d1b7f02629c\",\n",
    "        ),\n",
    "    ]\n",
    "else:\n",
    "    # Set path to data source\n",
    "    datasets.MNIST.mirrors = [\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/\"\n",
    "    ]\n",
    "\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    \"data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Randomly sample 16 images from the test set\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=16)\n",
    "test_images, _ = iter(test_loader).next()\n",
    "\n",
    "# inspect the images\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "    return\n",
    "\n",
    "\n",
    "# unnormalize the test images for displaying\n",
    "unnorm_images = (test_images * 0.3081) + 0.1307\n",
    "\n",
    "print(\"Sampled test images: \")\n",
    "imshow(torchvision.utils.make_grid(unnorm_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the sampled images to endpoint for inference\n",
    "outputs = predictor.predict(test_images.numpy())\n",
    "predicted = np.argmax(outputs, axis=1)\n",
    "\n",
    "print(\"Predictions: \")\n",
    "print(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
