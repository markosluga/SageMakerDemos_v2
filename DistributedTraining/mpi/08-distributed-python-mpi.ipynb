{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training using MPI on Amazon SageMaker\n",
    "\n",
    "***This notebook should be deployed in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel in the Oregon (us-west-2) region for the hyperllinks to the console to work correctly***\n",
    "\n",
    "[Open MPI](https://mpi4py.readthedocs.io/en/stable/overview.html) is a tool that allows us to convert a single-threaded python program into a parallel python program. \n",
    "\n",
    "SageMaker can use MPI to deploy multiple python threads on a single instance, or set up multiple instances with multiple threads each. In this demo we will deploy **multiple instances** that will run 2 parallel processes each. \n",
    "\n",
    "The mpi_demo.py python script provided in this demo ensures each process spawned looks for other processes and communicates with them. \n",
    "\n",
    "If the process can not communicate wiht other spawned processes it will fail and thus the training job would fail aswell. \n",
    "\n",
    "This functionality deonstrates that a single threaded process is being parallelized, which means you can import your existing single threaded training scripts and paralelize them with ease. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's deploy MPI!\n",
    "\n",
    "In the code below, note the **`distribution`** enables the use of MPI. Here, we can define the number of processes per host, which is set to 2.\n",
    "\n",
    "The number of instances is defined in the training configuration with **`instance_count`** which is set to 10.\n",
    "\n",
    "We are thus looking for 10 processes to be spawned and start communicating with each other.\n",
    "\n",
    "To prove that this is happening, look for the following message in the output of the next code run:\n",
    "\n",
    "**`[1,0]<stdout>:Number of MPI processes that will talk to each other: 10[1,0]<stdout>:`**\n",
    "\n",
    "Feel free to also inspect the [training job](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs) once you have ran the code below. \n",
    "\n",
    "Feel free to change the **`processes_per_host`**, **`instance_type`** and **`instacne_count`** if you would like to test a different number of distributed processes. Please keep the  **`instacne_count`** within your [account quotas](https://us-west-2.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas).\n",
    "\n",
    "This following code should take 3-4 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:08:40 Starting - Starting the training job...\n",
      "2022-06-16 17:09:04 Starting - Preparing the instances for trainingProfilerReport-1655399320: InProgress\n",
      ".........\n",
      "2022-06-16 17:10:24 Downloading - Downloading input data...\n",
      "2022-06-16 17:11:04 Training - Downloading the training image..\u001b[34m2022-06-16 17:11:20,655 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:20,662 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,137 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,161 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,170 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,171 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,179 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,180 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:21,181 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.174.26\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:20,830 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:20,838 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,255 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,374 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,388 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,389 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,396 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,460 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,460 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,460 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,460 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:21,472 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:22,182 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:22,182 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.174.26\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,302 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,310 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,818 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,843 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,857 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,857 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,858 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:20,859 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.180.202\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,865 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,959 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,959 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,960 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,960 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:21,972 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,189 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,298 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,299 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,299 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,305 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,413 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,413 sagemaker-training-toolkit INFO     Can connect to host algo-3\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,413 sagemaker-training-toolkit INFO     Worker algo-3 available for communication\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,419 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,485 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,485 sagemaker-training-toolkit INFO     Can connect to host algo-4\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,485 sagemaker-training-toolkit INFO     Worker algo-4 available for communication\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,491 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,558 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,558 sagemaker-training-toolkit INFO     Can connect to host algo-5\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,559 sagemaker-training-toolkit INFO     Worker algo-5 available for communication\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,559 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5'] Hosts: ['algo-1:2', 'algo-2:2', 'algo-3:2', 'algo-4:2', 'algo-5:2'] process_per_hosts: 2 num_processes: 10\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,560 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,567 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:23,578 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 2\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/ml/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2022-06-16-17-08-39-882\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-656165796789/tensorflow-training-2022-06-16-17-08-39-882/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_demo\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_demo.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mpi_demo.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-4\",\"algo-3\",\"algo-5\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mpi_demo\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-656165796789/tensorflow-training-2022-06-16-17-08-39-882/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\"],\"hyperparameters\":{\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2022-06-16-17-08-39-882\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-656165796789/tensorflow-training-2022-06-16-17-08-39-882/source/sourcedir.tar.gz\",\"module_name\":\"mpi_demo\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-4\",\"algo-3\",\"algo-5\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_demo.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:2,algo-2:2,algo-3:2,algo-4:2,algo-5:2 -np 10 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_MODEL_DIR -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py mpi_demo.py --model_dir /opt/ml/model\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:20,726 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:20,734 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,194 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,215 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,226 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,226 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,227 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:21,227 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.180.202\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,234 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,335 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,335 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,335 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,335 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:22,345 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:20,710 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:20,718 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,049 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,172 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,183 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,183 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,184 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:21,184 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.180.202\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,190 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,287 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,287 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,287 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,287 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:22,299 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m Data for JOB [61438,1] offset 0 Total slots allocated 10\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: ip-10-0-180-202#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 1 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 3 Bound: N/A\n",
      " Data for node: algo-3#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 5 Bound: N/A\n",
      " Data for node: algo-4#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-5#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [61438,1] App: 0 Process rank: 9 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Number of MPI processes that will talk to each other: 10[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:point to point\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:Hello I am rank 1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:I received some data: [1,1]<stdout>:[0 1 2]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:==================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Broadcasting from rank 0\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Data at rank 9 [1,9]<stdout>:[0 1 2 3 4 5 6 7 8 9][1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:Data at rank 5 [1,1]<stdout>:Data at rank 1 [1,1]<stdout>:[0 1 2 3 4 5 6 7 8 9][1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[0 1 2 3 4 5 6 7 8 9]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:Data at rank 3 [1,7]<stdout>:Data at rank 7 [1,7]<stdout>:[0 1 2 3 4 5 6 7 8 9]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[0 1 2 3 4 5 6 7 8 9][1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:Data at rank 4 [1,4]<stdout>:[0 1 2 3 4 5 6 7 8 9]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Data at rank 8 [1,8]<stdout>:[0 1 2 3 4 5 6 7 8 9][1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Data at rank 0 [1,0]<stdout>:[0 1 2 3 4 5 6 7 8 9]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:Data at rank 2[1,2]<stdout>: [1,2]<stdout>:[0 1 2 3 4 5 6 7 8 9][1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:Data at rank 6 [0 1 2 3 4 5 6 7 8 9]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:==================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Gather and reduce\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:I am rank 0, data I gathered is: [[0 0 0 0 0 0 0 0 0 0]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [1 1 1 1 1 1 1 1 1 1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [2 2 2 2 2 2 2 2 2 2]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [3 3 3 3 3 3 3 3 3 3]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [4 4 4 4 4 4 4 4 4 4]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [5 5 5 5 5 5 5 5 5 5]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [6 6 6 6 6 6 6 6 6 6]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [7 7 7 7 7 7 7 7 7 7]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [8 8 8 8 8 8 8 8 8 8]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: [9 9 9 9 9 9 9 9 9 9]]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:I am rank 0, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:I am rank 1, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:I am rank 2, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:I am rank 3, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5][1,5]<stdout>:I am rank 5, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:I am rank 9, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:I am rank 8, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:I am rank 4, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:I am rank 7, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:I am rank 6, my avg is: [4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5 4.5]\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:26,386 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2022-06-16 17:11:26,386 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-06-16 17:11:24 Training - Training image download completed. Training in progress.\u001b[35m2022-06-16 17:11:56,419 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:56,420 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[35mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[35mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[35m2022-06-16 17:11:56,420 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:56,452 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:56,452 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[33mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[33mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[33m2022-06-16 17:11:56,453 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:56,421 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:56,421 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[36mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[36mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[36m2022-06-16 17:11:56,421 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:56,417 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:56,417 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[32mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[32mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[32m2022-06-16 17:11:56,418 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-06-16 17:12:11 Uploading - Uploading generated training model\n",
      "2022-06-16 17:12:11 Completed - Training job completed\n",
      "Training seconds: 540\n",
      "Billable seconds: 540\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# distribution enables running 2 processes per host on 5 instances.\n",
    "\n",
    "distribution = {\"mpi\": {\"enabled\": True, \"processes_per_host\": 2}}\n",
    "\n",
    "tfest = TensorFlow(\n",
    "    entry_point=\"mpi_demo.py\",\n",
    "    role=role,\n",
    "    framework_version=\"2.3.0\",\n",
    "    distribution=distribution,\n",
    "    py_version=\"py37\",\n",
    "    instance_count=5,\n",
    "    instance_type=\"ml.c5.xlarge\",  # 4 cores\n",
    "    output_path=\"s3://\" + sagemaker.Session().default_bucket() + \"/\" + \"mpi\",\n",
    ")\n",
    "\n",
    "tfest.fit()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
