{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a SageMaker Built in Algorithm\n",
    "\n",
    "***Anomaly detection using Random Cut Forest (RCF) algorithm.***\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook should be deployed in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel in the Oregon (us-west-2) region for the hyperllinks to the console to work correctly* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "Amazon SageMaker Random Cut Forest (RCF) is an algorithm designed to detect anomalous data points within a dataset. It is a variant of the Cut Forest algorithm, adapted by AWS, that is avaialbe in the sagemaker python library. In this notebook, we will demonstrate the simplicity of using RCF to detect anomalies in the [Numenta Anomaly Benchmark (NAB) NYC Taxi dataset](https://github.com/numenta/NAB/blob/master/data/realKnownCause/nyc_taxi.csv), which records the amount New York City taxi ridership over the course of six months.\n",
    "\n",
    "The goal of this demo is to show how easy it is to utilize a built in SageMaker algorithm. As such, this notebook might not cover the full scope of the analysis that would be possible on this dataset.\n",
    "\n",
    "To gain deeper insight into the RCF algorithm please refer to the [SageMaker RCF Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the libraries\n",
    "\n",
    "***\n",
    "\n",
    "First we will import the libraries required to process the code:\n",
    "\n",
    "* boto3 - AWS SDK for Python\n",
    "* botocore - Boto3 core functionalities\n",
    "* sagemaker - The SageMaker Python SDK\n",
    "* sys - Python System-specific runtimes\n",
    "* pandas - Python data analysis and manipulation tool - used to create the dataframe that we will be working/visualizing from\n",
    "* matplotlib - a Python visualizations library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Output S3 Bucket\n",
    "\n",
    "In this scenario we will be using [the default SageMaker S3 bucket](https://s3.console.aws.amazon.com/s3/buckets?region=us-west-2&region=us-west-2), however feel free to change the destination bucket and prefix. The bucket is used to deliver the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket = (\n",
    "    sagemaker.Session().default_bucket()\n",
    ")\n",
    "prefix = \"sagemaker/rcf-demo\"\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "print(f\"Training input/output will be stored in: s3://{bucket}/{prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training data S3 Bucket\n",
    "\n",
    "Define the training data source bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "downloaded_data_bucket = f\"sagemaker-sample-files\"\n",
    "downloaded_data_prefix = \"datasets/tabular/anomaly_benchmark_taxi\"\n",
    "print(f\"Downloaded training data will be read from s3://{downloaded_data_bucket}/{downloaded_data_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Visualize the Example Data\n",
    "\n",
    "The sample data provided with this notebook (NAB_nyc_taxi.csv) consists of the number of New York City taxi passengers over the course of six months aggregated into 30-minute buckets. Within the data there are anomalous events occurring during the NYC marathon, Thanksgiving, Christmas, New Year's day, and on the day of a snow storm. \n",
    "\n",
    "Human beings are visual creatures and are able to very quickly identify anomalies when the data is visualised. \n",
    "\n",
    "In the next set of code we read the NAB_nyc_taxi.csv to create a dataframe using Pandas, and visualise the dataset with matplotlib, to see if we can spot some anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"NAB_nyc_taxi.csv\"\n",
    "taxi_data = pd.read_csv(data_filename, delimiter=\",\")\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "taxi_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With taxi ridership, the passenger count appears more or less periodic, however we can easily identify an anomaly at around datapoint number 6000. However, due to the nature of the human brain we tend to focus in on the very obvious anomalies, but are unable to detect any anomalies hidden to the huma eye.\n",
    "\n",
    "This is where the SageMaker RCF algorithm can really make a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Using RCF\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the built in RCF algorithm from the sagemaker python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RandomCutForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the training parameters:\n",
    "\n",
    "* **`role=execution_role`** - we are using the instance session role, however a custom role can be specified here\n",
    "* **`instance_count=1`** - number of instances for training\n",
    "* **`instance_type=\"ml.c5.4xlarge\"`** - type and size of the SageMaker training instance \n",
    "* **`data_location=f\"s3://{bucket}/{prefix}/\"`** - the output bucket defined in step XXX\n",
    "* **`output_path=f\"s3://{bucket}/{prefix}/output\"`** - the output bucket path\n",
    "\n",
    "  * Recommended instance type: `ml.m5.large` or `ml.c5.xlarge`\n",
    "  * The RCF algorithm does not take advantage of GPU hardware.\n",
    "\n",
    "Set up the hyperparameters:\n",
    "\n",
    "* **`num_samples_per_tree`** - the number randomly sampled data points sent to each tree. As a general rule, `1/num_samples_per_tree` should approximate the the estimated ratio of anomalies to normal points in the dataset.\n",
    "* **`num_trees`** - the number of trees to create in the forest. Each tree learns a separate model from different samples of data. The full forest model uses the mean predicted anomaly score from each constituent tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "rcf = RandomCutForest(\n",
    "    role=execution_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    data_location=f\"s3://{bucket}/{prefix}/\",\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    num_samples_per_tree=512,\n",
    "    num_trees=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training job - this should take about 3 minutes using an `ml.c5.xlarge` instance. - the status of the endpoint can be observed in the [SageMaker Console - Training Jobs](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf.fit(rcf.record_set(taxi_data.value.to_numpy().reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Using RCF\n",
    "\n",
    "***\n",
    "\n",
    "Now that we have a trained RCF model we can deploy a SageMaker endpoint to compute anomaly scores from our example data.\n",
    "\n",
    "To deploy the endpoint simply use th SageMaker Python SDK `deploy()` function from the model that was created in the training job. We only need to define the number of instances and the instance type for inference. The recommended  using the `ml.c5.large` instance type as it provides the fastest inference time at the lowest cost.\n",
    "\n",
    "Note: \n",
    "  * The default quota for your account might only allow the use of `ml.m5.large` instances for the endpoint deployment.\n",
    "  \n",
    "This process should take about 3 minutes - the status of the endpoint can be observed in the [SageMaker Console - Endpoints](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_inference = rcf.deploy(initial_instance_count=1, instance_type=\"ml.c5.xlarge\")\n",
    "print(f\"Endpoint name: {rcf_inference.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the training dataset to the endpoint we need to convert the CSV format to JSON. We can use the SageMaker serializer and deserializer, that can automatically take care of the datatype conversion from Numpy NDArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "rcf_inference.serializer = CSVSerializer()\n",
    "rcf_inference.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to run the inference on the data set and create the anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_numpy = taxi_data.value.to_numpy().reshape(-1, 1)\n",
    "results = rcf_inference.predict(taxi_data_numpy)\n",
    "scores = [datum[\"score\"] for datum in results[\"scores\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can plot the anomaly scores to the dataframe we created earlier and visualise the inference results to find andy hidden anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data[\"score\"] = pd.Series(scores, index=taxi_data.index)\n",
    "score_mean = taxi_data[\"score\"].mean()\n",
    "score_std = taxi_data[\"score\"].std()\n",
    "score_cutoff = score_mean + 3 * score_std\n",
    "start, end = 0, len(taxi_data)\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "anomalies = taxi_data_subset[taxi_data_subset[\"score\"] > score_cutoff]\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(anomalies.index, anomalies.score, \"ko\")\n",
    "ax1.plot(taxi_data_subset[\"value\"], color=\"C0\", alpha=0.8)\n",
    "ax2.plot(taxi_data_subset[\"score\"], color=\"C1\")\n",
    "ax1.grid(which=\"major\", axis=\"both\")\n",
    "ax1.set_ylabel(\"Taxi Ridership\", color=\"C0\")\n",
    "ax2.set_ylabel(\"Anomaly Score\", color=\"C1\")\n",
    "ax1.tick_params(\"y\", colors=\"C0\")\n",
    "ax2.tick_params(\"y\", colors=\"C1\")\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4 * max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using the RCF inference we are able to identify many additional anomalies in the dataset, that we would not be able to identify in a traditional visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the SageMaker Inference Endpoint\n",
    "\n",
    "To clean up, please uncomment the code below and run it to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker.Session().delete_endpoint(rcf_inference.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
